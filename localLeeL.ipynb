{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cf07f2c",
      "metadata": {
        "id": "8cf07f2c"
      },
      "source": [
        "# `localLeeL.py`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "# Only run this in Colab (itâ€™ll silently skip on your local machine)\n",
        "if 'google.colab' in sys.modules:\n",
        "    # 1) Download & unzip your repo from GitHub\n",
        "    !wget -q https://github.com/bobyellow/localBiT/archive/refs/heads/main.zip -O localBiT.zip\n",
        "    !unzip -q localBiT.zip\n",
        "    # 2) Change into the directory that was just created\n",
        "    %cd localBiT-main\n",
        "# 3) Ensure Python finds your modules\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Debug printout\n",
        "print(\"PWD:\", os.getcwd())\n",
        "print(\"Contents:\", os.listdir('.'))\n",
        "print(\"Has core?:\", os.path.isdir('core'))\n"
      ],
      "metadata": {
        "id": "u5RJfa4F0NXp",
        "outputId": "cbfc00d4-8f83-4a9c-b86e-6cc430e6e733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "u5RJfa4F0NXp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/localBiT-main\n",
            "PWD: /content/localBiT-main\n",
            "Contents: ['localLeeL.ipynb', 'localBiMoranI.py', 'localBiT.ipynb', 'Dockerfile', 'localMultiGearyC.ipynb', 'core', 'input', 'py2nb.py', 'result', 'LICENSE', 'localLeeL.py', 'localBiMoranI.ipynb', 'README.md', 'localBiT.py', 'localMultiGearyC.py']\n",
            "Has core?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25ffe9b8",
      "metadata": {
        "id": "25ffe9b8",
        "outputId": "68d01b54-91ac-4c26-ba51-0ff629c2acda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Complete.\n"
          ]
        }
      ],
      "source": [
        "\"\"\" local Lee's L (Lee 2001)\n",
        "Lee, S. I. (2001). Developing a bivariate spatial association measure: an integration of Pearson's r and Moran's I. Journal of Geographical Systems, 3(4), 369-385.\"\"\"\n",
        "\n",
        "import core.shapefile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from core.getNeighbors import getNeighborsAreaContiguity,extractCentroidsFromShapefile, kNearestNeighbors\n",
        "from core.spatstats import calculateLocalL\n",
        "\n",
        "# Load shapefile data\n",
        "sf = core.shapefile.Reader(\"input/Hex437.shp\") # Synthetic data\n",
        "#sf = core.shapefile.Reader(\"input/NPA_2014_Mecklenburg.shp\") # Case study with Mecklenburg County\n",
        "shapes = sf.shapes()\n",
        "\n",
        "# Prepare AREAS input for Queen's and Rook's contiguity\n",
        "AREAS = [[shape.points] for shape in shapes]  # Ensure proper structure for AREAS\n",
        "\n",
        "# Calculate neighbors using Queen's and Rook's contiguity\n",
        "Wqueen, Wrook = getNeighborsAreaContiguity(AREAS)\n",
        "neighbors = Wqueen  # Use Queen's contiguity for further analysis\n",
        "\n",
        "\"\"\"\n",
        "# Use k nearest neighbors instead of contiguity\n",
        "shapefile_path = \"input/Hex437.shp\"\n",
        "centroids = extractCentroidsFromShapefile(shapefile_path)\n",
        "\n",
        "# Compute k-nearest neighbors (adjust k as needed)\n",
        "k = 5  # Number of nearest neighbors\n",
        "neighbors = kNearestNeighbors(centroids, k=k)\n",
        "\"\"\"\n",
        "\n",
        "# Load and extract raw data from CSV\n",
        "data = pd.read_csv(\"input/Hex437noNorm.csv\") # Synthetic data\n",
        "#data = pd.read_csv(\"input/CLT_Data_Bac_Elem.csv\") # Case study with Mecklenburg County\n",
        "\n",
        "id = data.FID  # Unique IDs for spatial units\n",
        "var1 = data.X  # Variable 1 values in synthetic data\n",
        "var2 = data.Y  # Variable 2 values in synthetic data\n",
        "#var1 = data.Z2_Value90_50  # Variable 1 values in synthetic data\n",
        "#var2 = data.Z_Value90_50  # Variable 2 values in synthetic data\n",
        "Cluster = data.Cluster  # Cluster membership (targeted special zones in synthetic data)\n",
        "Edge = data.Edge  # Edge of special zones, useful for edge effect considerations\n",
        "\n",
        "\n",
        "#var1 = data.Bac  # Variable 1 values in Mecklenburg data: % of adults with bachelor's degree or higher\n",
        "#var2 = data.Elem  # Variable 2 values in synthetic data: average test scores of elementary school students\n",
        "\n",
        "\n",
        "# Create a dictionary mapping IDs to variables\n",
        "dataDictionary = {int(b): [var1[a], var2[a]] for a, b in enumerate(id)}\n",
        "areaKeys = list(dataDictionary.keys())\n",
        "\n",
        "# Standardize variables\n",
        "var1_std = [(val - np.mean(var1)) / np.std(var1) for val in var1]\n",
        "var2_std = [(val - np.mean(var2)) / np.std(var2) for val in var2]\n",
        "dataDictionary_std = {int(b): [var1_std[a], var2_std[a]] for a, b in enumerate(id)}\n",
        "\n",
        "\n",
        "# Compute (bivariate) L for each spatial unit\n",
        "BLValues = {}\n",
        "result = []\n",
        "for x in areaKeys:\n",
        "    keyList = neighbors[x]\n",
        "    currentBL = calculateLocalL(x, keyList, dataDictionary_std)\n",
        "    result.append(currentBL)\n",
        "    BLValues[x] = currentBL\n",
        "\n",
        "# Monte Carlo permutation for significance test\n",
        "plist = []\n",
        "pvalue = []\n",
        "dataLength = len(shapes)\n",
        "\n",
        "for x in areaKeys:\n",
        "    Nlist = list(range(dataLength))\n",
        "    betterClusters = 0\n",
        "    number = len(neighbors[x])\n",
        "    Nlist.remove(x)\n",
        "    for _ in range(999):  # Perform 999 random permutations\n",
        "        permKey = np.random.choice(Nlist, number, replace=False)\n",
        "        randomBL = calculateLocalL(x, permKey, dataDictionary_std)\n",
        "        if BLValues[x] > randomBL:\n",
        "            betterClusters += 1\n",
        "    p = (betterClusters + 1) / 1000.0 # The most important result. The highest/lowest ones correspond to positive/negative clusters of bivariate spatial association\n",
        "    plist.append(p)\n",
        "    pvalue.append(p if p < 0.5 else 1 - p) # Adjust positive cluster's p-value to small values\n",
        "\n",
        "# Identify patterns at different significance levels\n",
        "idx = []\n",
        "dataMean = np.mean(np.double(var1))\n",
        "dataMean2 = np.mean(np.double(var2))\n",
        "\n",
        "for x in areaKeys:\n",
        "    if plist[x] >= 0.95: # Modify to 0.99 (0.999) for p-value < 0.01 (0.001) level\n",
        "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] < dataMean2:\n",
        "            idx.append('LL')\n",
        "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] > dataMean2:\n",
        "            idx.append('HH')\n",
        "        else:\n",
        "            idx.append('OtherBgL')\n",
        "    elif plist[x] <= 0.05:\n",
        "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] > dataMean2:\n",
        "            idx.append('LH')\n",
        "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] < dataMean2:\n",
        "            idx.append('HL')\n",
        "        else:\n",
        "            idx.append('OtherSmL')\n",
        "    else:\n",
        "        idx.append('NS')\n",
        "\n",
        "\n",
        "# Save results to a DataFrame and export to CSV\n",
        "df = pd.DataFrame({\n",
        "    'OBJECTID': id + 1,  # Add 1 to IDs for compatibility\n",
        "    'Cluster': Cluster,\n",
        "    'Edge': Edge,\n",
        "    'var1': var1,  # Original Variable 1 values\n",
        "    'var2': var2,  # Original Variable 2 values\n",
        "    'L': result,  # Computed Lee's L statistic\n",
        "    'p_sim': plist,  # Pseudo p-values\n",
        "    'p_value': pvalue,  # Adjusted p-values\n",
        "    'pattern': idx,  # Patterns that passed the significant test\n",
        "})\n",
        "\n",
        "df.to_csv(\"result/LeeL_Hex437_XY_Quali.csv\", index=False)\n",
        "#df.to_csv(\"result/BiT_Hex437_Z2Z_Quali_10_50_90.csv\", index=False)\n",
        "#df.to_csv(\"result/BiT_Meck_Bac_ELem.csv\", index=False)\n",
        "print(\"Processing Complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}