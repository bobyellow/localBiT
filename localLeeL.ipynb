{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf07f2c",
   "metadata": {},
   "source": [
    "# `localLeeL.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffe9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" local Lee's L (Lee 2001)\n",
    "Lee, S. I. (2001). Developing a bivariate spatial association measure: an integration of Pearson's r and Moran's I. Journal of Geographical Systems, 3(4), 369-385.\"\"\"\n",
    "\n",
    "import core.shapefile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from core.getNeighbors import getNeighborsAreaContiguity,extractCentroidsFromShapefile, kNearestNeighbors\n",
    "from core.spatstats import calculateLocalL\n",
    "\n",
    "# Load shapefile data\n",
    "sf = core.shapefile.Reader(\"input/Hex437.shp\") # Synthetic data\n",
    "#sf = core.shapefile.Reader(\"input/NPA_2014_Mecklenburg.shp\") # Case study with Mecklenburg County \n",
    "shapes = sf.shapes()\n",
    "\n",
    "# Prepare AREAS input for Queen's and Rook's contiguity\n",
    "AREAS = [[shape.points] for shape in shapes]  # Ensure proper structure for AREAS\n",
    "\n",
    "# Calculate neighbors using Queen's and Rook's contiguity\n",
    "Wqueen, Wrook = getNeighborsAreaContiguity(AREAS)\n",
    "neighbors = Wqueen  # Use Queen's contiguity for further analysis\n",
    "\n",
    "\"\"\"\n",
    "# Use k nearest neighbors instead of contiguity\n",
    "shapefile_path = \"input/Hex437.shp\"\n",
    "centroids = extractCentroidsFromShapefile(shapefile_path)\n",
    "\n",
    "# Compute k-nearest neighbors (adjust k as needed)\n",
    "k = 5  # Number of nearest neighbors\n",
    "neighbors = kNearestNeighbors(centroids, k=k)\n",
    "\"\"\"\n",
    "\n",
    "# Load and extract raw data from CSV\n",
    "data = pd.read_csv(\"input/Hex437noNorm.csv\") # Synthetic data\n",
    "#data = pd.read_csv(\"input/CLT_Data_Bac_Elem.csv\") # Case study with Mecklenburg County \n",
    "\n",
    "id = data.FID  # Unique IDs for spatial units\n",
    "var1 = data.X  # Variable 1 values in synthetic data\n",
    "var2 = data.Y  # Variable 2 values in synthetic data\n",
    "#var1 = data.Z2_Value90_50  # Variable 1 values in synthetic data\n",
    "#var2 = data.Z_Value90_50  # Variable 2 values in synthetic data\n",
    "Cluster = data.Cluster  # Cluster membership (targeted special zones in synthetic data)\n",
    "Edge = data.Edge  # Edge of special zones, useful for edge effect considerations\n",
    "\n",
    "\n",
    "#var1 = data.Bac  # Variable 1 values in Mecklenburg data: % of adults with bachelor's degree or higher\n",
    "#var2 = data.Elem  # Variable 2 values in synthetic data: average test scores of elementary school students\n",
    "\n",
    "\n",
    "# Create a dictionary mapping IDs to variables\n",
    "dataDictionary = {int(b): [var1[a], var2[a]] for a, b in enumerate(id)}\n",
    "areaKeys = list(dataDictionary.keys())\n",
    "\n",
    "# Standardize variables\n",
    "var1_std = [(val - np.mean(var1)) / np.std(var1) for val in var1]\n",
    "var2_std = [(val - np.mean(var2)) / np.std(var2) for val in var2]\n",
    "dataDictionary_std = {int(b): [var1_std[a], var2_std[a]] for a, b in enumerate(id)}\n",
    "\n",
    "\n",
    "# Compute (bivariate) L for each spatial unit\n",
    "BLValues = {}\n",
    "result = []\n",
    "for x in areaKeys:\n",
    "    keyList = neighbors[x]\n",
    "    currentBL = calculateLocalL(x, keyList, dataDictionary_std)\n",
    "    result.append(currentBL)\n",
    "    BLValues[x] = currentBL\n",
    "\n",
    "# Monte Carlo permutation for significance test\n",
    "plist = []\n",
    "pvalue = []\n",
    "dataLength = len(shapes)\n",
    "\n",
    "for x in areaKeys:\n",
    "    Nlist = list(range(dataLength))\n",
    "    betterClusters = 0\n",
    "    number = len(neighbors[x])\n",
    "    Nlist.remove(x)\n",
    "    for _ in range(999):  # Perform 999 random permutations\n",
    "        permKey = np.random.choice(Nlist, number, replace=False)\n",
    "        randomBL = calculateLocalL(x, permKey, dataDictionary_std)\n",
    "        if BLValues[x] > randomBL:\n",
    "            betterClusters += 1\n",
    "    p = (betterClusters + 1) / 1000.0 # The most important result. The highest/lowest ones correspond to positive/negative clusters of bivariate spatial association\n",
    "    plist.append(p)\n",
    "    pvalue.append(p if p < 0.5 else 1 - p) # Adjust positive cluster's p-value to small values\n",
    "\n",
    "# Identify patterns at different significance levels\n",
    "idx = []\n",
    "dataMean = np.mean(np.double(var1))\n",
    "dataMean2 = np.mean(np.double(var2))\n",
    "\n",
    "for x in areaKeys:\n",
    "    if plist[x] >= 0.95: # Modify to 0.99 (0.999) for p-value < 0.01 (0.001) level\n",
    "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] < dataMean2:\n",
    "            idx.append('LL')\n",
    "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] > dataMean2:\n",
    "            idx.append('HH')\n",
    "        else:\n",
    "            idx.append('OtherBgL')\n",
    "    elif plist[x] <= 0.05:\n",
    "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] > dataMean2:\n",
    "            idx.append('LH')\n",
    "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] < dataMean2:\n",
    "            idx.append('HL')\n",
    "        else:\n",
    "            idx.append('OtherSmL')\n",
    "    else:\n",
    "        idx.append('NS')\n",
    "\n",
    "\n",
    "# Save results to a DataFrame and export to CSV\n",
    "df = pd.DataFrame({\n",
    "    'OBJECTID': id + 1,  # Add 1 to IDs for compatibility\n",
    "    'Cluster': Cluster,\n",
    "    'Edge': Edge,\n",
    "    'var1': var1,  # Original Variable 1 values\n",
    "    'var2': var2,  # Original Variable 2 values\n",
    "    'L': result,  # Computed Lee's L statistic\n",
    "    'p_sim': plist,  # Pseudo p-values\n",
    "    'p_value': pvalue,  # Adjusted p-values\n",
    "    'pattern': idx,  # Patterns that passed the significant test\n",
    "})\n",
    "\n",
    "df.to_csv(\"result/LeeL_Hex437_XY_Quali.csv\", index=False)\n",
    "#df.to_csv(\"result/BiT_Hex437_Z2Z_Quali_10_50_90.csv\", index=False)\n",
    "#df.to_csv(\"result/BiT_Meck_Bac_ELem.csv\", index=False)\n",
    "print(\"Processing Complete.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
