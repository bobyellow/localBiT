{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de54dbae",
   "metadata": {},
   "source": [
    "# `localBiMoranI.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129dfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" local Bivariate Moran's I (Anselin et al. 2002) (Also available in GeoDa)\n",
    "Anselin, L., Syabri, I., & Smirnov, O. (2002). Visualizing multivariate spatial correlation with dynamically linked windows. In Proceedings, CSISS Workshop on New Tools for Spatial Data Analysis, Santa Barbara, CA.\n",
    "\"\"\"\n",
    "\"\"\"the order of two variables matter\"\"\"\n",
    "import core.shapefile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from core.getNeighbors import getNeighborsAreaContiguity,extractCentroidsFromShapefile, kNearestNeighbors\n",
    "from core.spatstats import calculateBivaraiteMoranI\n",
    "\n",
    "# Load shapefile data\n",
    "sf = core.shapefile.Reader(\"input/Hex437.shp\") # Synthetic data\n",
    "#sf = core.shapefile.Reader(\"input/NPA_2014_Mecklenburg.shp\") # Case study with Mecklenburg County \n",
    "shapes = sf.shapes()\n",
    "\n",
    "# Prepare AREAS input for Queen's and Rook's contiguity\n",
    "AREAS = [[shape.points] for shape in shapes]  # Ensure proper structure for AREAS\n",
    "\n",
    "# Calculate neighbors using Queen's and Rook's contiguity\n",
    "Wqueen, Wrook = getNeighborsAreaContiguity(AREAS)\n",
    "neighbors = Wqueen  # Use Queen's contiguity for further analysis\n",
    "\n",
    "\"\"\"\n",
    "# Use k nearest neighbors instead of contiguity\n",
    "shapefile_path = \"input/Hex437.shp\"\n",
    "centroids = extractCentroidsFromShapefile(shapefile_path)\n",
    "\n",
    "# Compute k-nearest neighbors (adjust k as needed)\n",
    "k = 5  # Number of nearest neighbors\n",
    "neighbors = kNearestNeighbors(centroids, k=k)\n",
    "\"\"\"\n",
    "\n",
    "# Load and extract raw data from CSV\n",
    "data = pd.read_csv(\"input/Hex437noNorm.csv\") # Synthetic data\n",
    "#data = pd.read_csv(\"input/CLT_Data_Bac_Elem.csv\") # Case study with Mecklenburg County \n",
    "\n",
    "id = data.FID  # Unique IDs for spatial units\n",
    "var1 = data.X  # Variable 1 values in synthetic data\n",
    "var2 = data.Y  # Variable 2 values in synthetic data\n",
    "#var1 = data.Z2_Value90_50  # Variable 1 values in synthetic data\n",
    "#var2 = data.Z_Value90_50  # Variable 2 values in synthetic data\n",
    "Cluster = data.Cluster  # Cluster membership (targeted special zones in synthetic data)\n",
    "Edge = data.Edge  # Edge of special zones, useful for edge effect considerations\n",
    "\n",
    "\n",
    "#var1 = data.Bac  # Variable 1 values in Mecklenburg data: % of adults with bachelor's degree or higher\n",
    "#var2 = data.Elem  # Variable 2 values in synthetic data: average test scores of elementary school students\n",
    "\n",
    "\n",
    "# Create a dictionary mapping IDs to variables\n",
    "dataDictionary = {int(b): [var1[a], var2[a]] for a, b in enumerate(id)}\n",
    "areaKeys = list(dataDictionary.keys())\n",
    "\n",
    "# Standardize variables\n",
    "var1_std = [(val - np.mean(var1)) / np.std(var1) for val in var1]\n",
    "var2_std = [(val - np.mean(var2)) / np.std(var2) for val in var2]\n",
    "dataDictionary_std = {int(b): [var1_std[a], var2_std[a]] for a, b in enumerate(id)}\n",
    "\n",
    "\n",
    "# Compute BI for each spatial unit\n",
    "BIValues = {}\n",
    "result = []\n",
    "for x in areaKeys:\n",
    "    keyList = neighbors[x]\n",
    "    currentBI = calculateBivaraiteMoranI(x, keyList, dataDictionary_std)\n",
    "    result.append(currentBI)\n",
    "    BIValues[x] = currentBI\n",
    "\n",
    "# Monte Carlo permutation for significance test\n",
    "plist = []\n",
    "pvalue = []\n",
    "dataLength = len(shapes)\n",
    "\n",
    "for x in areaKeys:\n",
    "    Nlist = list(range(dataLength))\n",
    "    betterClusters = 0\n",
    "    number = len(neighbors[x])\n",
    "    Nlist.remove(x)\n",
    "    for _ in range(999):  # Perform 999 random permutations\n",
    "        permKey = np.random.choice(Nlist, number, replace=False)\n",
    "        randomBI = calculateBivaraiteMoranI(x, permKey, dataDictionary_std)\n",
    "        if BIValues[x] > randomBI:\n",
    "            betterClusters += 1\n",
    "    p = (betterClusters + 1) / 1000.0 # The most important result. The highest/lowest ones correspond to positive/negative clusters of bivariate spatial association\n",
    "    plist.append(p)\n",
    "    pvalue.append(p if p < 0.5 else 1 - p) # Adjust positive cluster's p-value to small values\n",
    "\n",
    "# Identify patterns at different significance levels\n",
    "idx = []\n",
    "dataMean = np.mean(np.double(var1))\n",
    "dataMean2 = np.mean(np.double(var2))\n",
    "\n",
    "for x in areaKeys:\n",
    "    if plist[x] >= 0.95: #modify to 0.99 (0.999) for p-value < 0.01 (0.001) level\n",
    "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] < dataMean2:\n",
    "            idx.append('LL')\n",
    "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] > dataMean2:\n",
    "            idx.append('HH')\n",
    "        else:\n",
    "            idx.append('OtherBgI')\n",
    "    elif plist[x] <= 0.05:\n",
    "        if dataDictionary[x][0] < dataMean and dataDictionary[x][1] > dataMean2:\n",
    "            idx.append('LH')\n",
    "        elif dataDictionary[x][0] > dataMean and dataDictionary[x][1] < dataMean2:\n",
    "            idx.append('HL')\n",
    "        else:\n",
    "            idx.append('OtherSmI')\n",
    "    else:\n",
    "        idx.append('NS')\n",
    "\n",
    "\n",
    "# Save results to a DataFrame and export to CSV\n",
    "df = pd.DataFrame({\n",
    "    'OBJECTID': id + 1,  # Add 1 to IDs for compatibility\n",
    "    'Cluster': Cluster,\n",
    "    'Edge': Edge,\n",
    "    'var1': var1,  # Original Variable 1 values\n",
    "    'var2': var2,  # Original Variable 2 values\n",
    "    'BI': result,  # Computed BiMoranI statistic\n",
    "    'p_sim': plist,  # Pseudo p-values\n",
    "    'p_value': pvalue,  # Adjusted p-values\n",
    "    'pattern': idx,  # Patterns that passed the significant test\n",
    "})\n",
    "\n",
    "df.to_csv(\"result/BiMoranI_Hex437_XY_Quali.csv\", index=False)\n",
    "#df.to_csv(\"result/BiT_Hex437_Z2Z_Quali_10_50_90.csv\", index=False)\n",
    "#df.to_csv(\"result/BiT_Meck_Bac_ELem.csv\", index=False)\n",
    "print(\"Processing Complete.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
